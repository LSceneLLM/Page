
<!-- saved from url=(0020)https://www.lerf.io/ -->
<html class="wf-opensans-n4-active wf-opensans-n6-active wf-opensans-n7-active wf-opensans-n8-active wf-roboto-n4-active wf-varelaround-n4-active wf-lato-n1-active wf-lato-n3-active wf-lato-i1-active wf-lato-n4-active wf-lato-i4-active wf-lato-n7-active wf-roboto-n3-active wf-roboto-n5-active wf-opensans-n3-active wf-opensans-i3-active wf-opensans-i8-active wf-opensans-i7-active wf-opensans-i6-active wf-opensans-i4-active wf-ubuntu-i4-active wf-ubuntu-n4-active wf-ubuntu-i3-active wf-lato-n9-active wf-ubuntu-n3-active wf-lato-i3-active wf-lato-i9-active wf-lato-i7-active wf-montserrat-i6-active wf-montserrat-i7-active wf-montserrat-i9-active wf-montserrat-i8-active wf-montserrat-i5-active wf-montserrat-i2-active wf-montserrat-i1-active wf-montserrat-i4-active wf-montserrat-i3-active wf-montserrat-n5-active wf-montserrat-n7-active wf-montserrat-n9-active wf-montserrat-n2-active wf-montserrat-n4-active wf-montserrat-n6-active wf-montserrat-n3-active wf-montserrat-n1-active wf-montserrat-n8-active wf-bungeeshade-n4-active wf-bungeeoutline-n4-active wf-changaone-i4-active wf-changaone-n4-active wf-ubuntu-n7-active wf-ubuntu-n5-active wf-ubuntu-i5-active wf-ubuntu-i7-active wf-active"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <!-- Google tag (gtag.js) -->
    <script async="" src="./template/js"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'G-FYQNRK8LHK');
    </script>

    
    <title>LSceneLLM: Empower Large 3D Scene Understanding with Hierarchical Modeling</title>

    <!-- <meta content="Grounding CLIP vectors volumetrically inside a NeRF allows flexible natural language queries in 3D" name="description">
    <meta content="FGPrompt: Fine-grained Goal Prompting for Image-goal Navigation" property="og:title">
    <meta content="Grounding CLIP vectors volumetrically inside a NeRF allows flexible natural language queries in 3D" property="og:description">
    <meta content="http://lerf.io/data/lerf_meta_img.jpg" property="og:image">
    <meta content="FGPrompt: Fine-grained Goal Prompting for Image-goal Navigation" property="twitter:title">
    <meta content="Grounding CLIP vectors volumetrically inside a NeRF allows flexible natural language queries in 3D" property="twitter:description">
    <meta content="http://lerf.io/data/lerf_meta_img.jpg" property="twitter:image">
    <meta property="og:type" content="website">
    <meta content="summary_large_image" name="twitter:card">
    <meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1"> -->


    <link href="https://fonts.googleapis.com/" rel="preconnect">
    <link href="https://fonts.gstatic.com/" rel="preconnect" crossorigin="anonymous">
    <script src="./template/webfont.js" type="text/javascript"></script>
    <link rel="stylesheet" href="./template/css" media="all"><script type="text/javascript">WebFont.load({ google: { families: ["Lato:100,100italic,300,300italic,400,400italic,700,700italic,900,900italic", "Montserrat:100,100italic,200,200italic,300,300italic,400,400italic,500,500italic,600,600italic,700,700italic,800,800italic,900,900italic", "Ubuntu:300,300italic,400,400italic,500,500italic,700,700italic", "Open Sans:300,300italic,400,400italic,600,600italic,700,700italic,800,800italic", "Changa One:400,400italic", "Varela Round:400", "Bungee Shade:regular", "Roboto:300,regular,500", "Bungee Outline:regular"] } });</script>
    <!--[if lt IE 9]><script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js" type="text/javascript"></script><![endif]-->

    <script src="./template/jquery.min.js"></script>
    <script src="./template/script.js" type="text/javascript"></script>

    <link href="./template/style.css" rel="stylesheet" type="text/css">

    <link href="data/robot.svg" rel="shortcut icon" type="image/x-icon">

    <link rel="stylesheet" href="./template/font-awesome.min.css">
</head>

<body data-new-gr-c-s-check-loaded="14.1130.0" data-gr-ext-installed="">
    <div class="section">
        <div class="container">
            <div class="title-row">
                <h1 class="title">LSceneLLM</h1>
                <h1 class="subheader">Empower Large 3D Scene Understanding with Hierarchical Modeling</h1>
            </div>
            <!-- <div class="base-row author-row">
                <div class="base-col author-col">
                    <a href="https://github.com/XinyuSun" target="_blank" class="author-text">
                        <sup>1,3</sup>Xinyu Sun
                    </a>
                </div>
                <div class="base-col author-col">
                    <a href="https://peihaochen.github.io/" target="_blank" class="author-text">
                        <sup>1</sup>Peihao Chen
                    </a>
                </div>
                <div class="base-col author-col">
                    <a href="https://github.com/XinyuSun" target="_blank" class="author-text">
                        <sup>1</sup>Jugang Fan
                    </a>
                </div>
                <div class="base-col author-col">
                    <a href="https://ieeexplore.ieee.org/author/37086497292" target="_blank" class="author-text">
                        <sup>4</sup>Thomas H. Li
                        <span class="superscript"></span>
                    </a>
                </div>
                <div class="base-col author-col">
                    <a href="https://orcid.org/0000-0003-4769-1526" target="_blank" class="author-text">
                        <sup>1</sup>Jian Chen
                    </a>
                </div>
                <div class="base-col author-col">
                    <a href="https://tanmingkui.github.io/" target="_blank" class="author-text">
                        <sup>1,2,5</sup>Mingkui Tan
                    </a>
                </div>
            </div>
            <div>
                <h1 id="uc-berkeley"><sup>1</sup>South China University of Technology, <sup>2</sup>Pazhou Laboratory</h1>
                <h1 id="uc-berkeley"><sup>3</sup>Information Technology R&D Innovation Center of Peking University</h1>
                <h1 id="uc-berkeley"><sup>4</sup>Peking University Shenzhen Graduate School</h1>
                <h1 id="uc-berkeley"><sup>5</sup>Key Laboratory of Big Data and Intelligent Robot, Ministry of Education</h1>
                <!-- <br /> -->
                <!-- <div id="equal_contrib">
                    <span class="text-star">*</span>Denotes Equal Contribution
                </div> -->

            <!-- </div> --> 

            <div class="title-row">
                <h2 class="subheader">Arxiv 2024</h2>
            </div>

            <div class="link-labels base-row">
                <!-- TODO: Update arxiv link -->
                <div class="base-col icon-col"><a href="https://arxiv.org/abs/2310.07473" target="_blank" class="link-block">
                    <img src="./template/5cab99df4998decfbf9e218e_paper-01.png" alt="paper" sizes="(max-width: 479px) 12vw, (max-width: 767px) 7vw, (max-width: 991px) 41.8515625px, 56.6953125px" src="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/5cab99df4998decfbf9e218e_paper-01-p-500.png" class="icon-img"></a></div>
                <!-- TODO: Update code link -->
                <div class="base-col icon-col"><a href="https://github.com/XinyuSun/FGPrompt" class="link-block"><img src="./template/5cae3b53b42ebb3dd4175a82_68747470733a2f2f7777772e69636f6e66696e6465722e636f6d2f646174612f69636f6e732f6f637469636f6e732f313032342f6d61726b2d6769746875622d3235362e706e67.png" alt="paper" class="icon-img github-img-icon"></a></div>
                <!-- TODO: Update data link -->
                <div class="column-2 base-col icon-col"><a href="" target="_blank" class="link-block"><img src="./template/5e7136849ee3b0a0c6a95151_database.svg" alt="paper" class="icon-img data-img-icon"></a></div>
            </div>
            <div class="link-labels base-row">
                <div class="base-col icon-col">
                    <strong class="link-labels-text">Paper</strong>
                </div>
                <div class="base-col icon-col">
                    <strong class="link-labels-text">&lt;/Code&gt;</strong>
                </div>
                <div class="base-col icon-col">
                    <strong class="link-labels-text">Data</strong>
                </div>
            </div>
            <h1 class="tldr">
                <b>TL;DR</b>:
                Hierarchical scene modeling allows effective large environment perception.
                <!-- We proposed a fine-grained goal prompting method for image-goal navigation. It significantly outperforms baselines (+209%), surpassing SOTA by 8% in success rate with 1/50 model size. -->
            </h1>
            <video id="main-video" autobuffer="" muted="" autoplay="" loop="" controls="">
                <source id="mp4" src="./new_data/page.mp4" type="video/mp4">
            </video>


            <div class="base-row add-top-padding">
                <h1 id="abstract">Overview</h1>
                <p class="paragraph">
                    Research on 3D Vision-Language Models (3D-VLMs) is gaining increasing attention.  
                    Due to the high density of visual features in 3D scenes, there are still challenges in developing 3D-VLMs: 
                    1) The sample-based approach results in a <b>loss of detailed scene information</b> due to aggressive downsampling. 
                    2) The object-centric method demands dense features for each object from the entire scene, leading to a <b>significant computational burden</b>.
                    To balance computational resources with preserving scene details, we propose <b>LSceneLLM</b>, a hierarchical framework that first performs region-level modeling for large scenes, followed by a plug-and-play scene magnifier module to capture fine-grained details in focused regions.
                    To comprehensively evaluate the <b>large scene understanding ability</b> of 3D-VLMs, we further introduce a cross-room understanding benchmark, <b>XR-Scene</b>, which contains a series of scene understanding tasks including XR-QA, XR-EmbodiedPlanning, and XR-SceneCaption. 
                    The average scene size in XR-Scene is <b>132m^2</b>, significantly larger than the 29m^2 of ScanQA.
                    Experiments show that our method achieves remarkable results and surpasses existing methods on both large scene understanding and existing scene understanding benchmarks.
                </p>
                <img class="pipeline-img" src="./new_data/teaser.png" style="max-width: 100%; height: auto;">
                <figcaption>
                    <b>(a)</b>: Existing methods suffer from scene details loss or heavy computation when facing large scenes.
                    <b>(b)</b>: We are committed to finding a balance between computational resources and preserving scene details through hierarchical modeling of the scene.
                </figcaption>
            </div>

            <!-- <div id="why-lerf" style="display: flex; align-items: flex-start;">
                <div style="flex: 1; padding-right: 0px;">
                    <h1 style="text-align: center;">Main Results</h1>
                    <p class="paragraph">
                        Our experimental results demonstrate that LSceneLLM achieves state-of-the-art performance on a wide range of 3D tasks and benchmarks, including single-room scene benchmarks and large scene benchmarks.
                    </p>
                </div>
                <div style="flex: 1; display: flex; flex-direction: column; align-items: flex-end;">
                    <img id="pipeline-img" src="./new_data/soart.png" style="max-width: 100%; height: auto;">
                </div>
            </div> -->


                <!-- <div id="why-lerf">
                    <h1> Main Results</h1>
                    <p class="paragraph">
                        Compared with existing methods on the image-goal navigation benchmark, our method brings significant performance improvement on 3 benchmark datasets (i.e., Gibson, MP3D, and HM3D). 
                        Especially on Gibson, we significantly <b>outperform baselines by +209%</b> and surpass the state-of-the-art success rate by a large margin <b>with only 1/50 model size</b>.
                    </p>
                    <img id="pipeline-img" src="./new_data/teaser.png" style="width: 100%; height: auto;">
                    <figcaption><b>left</b>: success rate comparison with baseline on three different datasets; <b>right</b>: comparison with SOTA both on success rate and the number of parameters.</figcaption>
                </div> -->
                <div id="why-lerf">
                    <h1> Proposed Method</h1>
                    <p class="paragraph">
                        The principle behind LSceneLLM is straightforward: rather than extracting all objects from a large scene to create visual features, 
                        we propose fisrt obtaining a coarse understanding of the entire scene, and dynamically selected dense scene details of focus region through <b>scene magnifier module</b>. 
                        Specifically, the raw scene point cloud is downsampled to produce a sparse version. 
                        These sparse vision tokens are concatenated with text tokens and fed into the LLM. 
                        The attention map generated by the LLM's self-attention module is then utilized to select a subset of important dense vision tokens.
                    </p>
                    <!-- <video id="main-video" autobuffer="" muted="" autoplay="" loop="" controls="">
                        <source id="mp4" src="data/video/method.mp4" type="video/mp4">
                    </video> -->
                    <img id="method-img" src="./new_data/method.png" style="max-width: 100%; height: auto;">
        
                    <p class="paragraph">
                        This scene magnifier module includes two sub-modules: dense vision token selector and adaptive self-attention module.
                        In dense vision token selector, regions that corresponding to the highest attention values are selected to obtain dense vision features,  
                        which are then sent to the adaptive self-attention module in the next layer. 
                        The adaptive self-attention module, which replaces the original self-attention module, 
                        computes the attention between the selected dense vision tokens, 
                        sparse vision tokens, and text tokens. 
                        This allows the model to focus on important regions in the large scene and efficiently obtain detailed information.
                    </p>

                    <img id="method-img" src="./new_data/detail_method.png" style="max-width: 100%; height: auto;">
                    
                </div>

                <div class="why-lerf">
                    <h1>SOTA Comparison on Various 3D Understanding Benchmarks</h1>
                    <p class="paragraph">
                        Our approach consistently demonstrates superior performance in both indoor and outdoor large-scene under-
                        standing, achieving outstanding results across a wide
                        range of 3D understanding benchmarks.
                    </p>
                </div>
                <img class="pipeline-img" src="./new_data/result.png" style="width: 100%;">
                <!-- <img id="pipeline-img" src="./data/cross-domain-table.png"> -->

                <div id="why-lerf">
                    <h1> Attention Map Visualization</h1>
                    <p class="paragraph">
                        We explore the areas the model focuses on when answering questions by visualizing the attention maps of the generated sequence of LLM to scene vision tokens.
                        LSceneLLM can accurately identifies the position of question-related objects in large scene.
                    </p>
                    <img class="heatmap-img" src="./new_data/viz.png" style="max-width: 100%; height: auto;">
                    <!-- <img id="why-lerf-img" src="./data/supp-early-visual_00.png"> -->
                    <!-- <figcaption><b>left</b>: EigenCAM visualization of the <b>Mid Fusion</b> activation maps; <b>right</b>: EigenCAM visualization of the <b>Early Fusion</b> activation maps.</figcaption> -->
                </div>

            

                <!-- <div class="highlight-none notranslate"><div class="highlight"><pre id="codecell0"><span></span>@article{nerfstudio, -->
                <!-- <div class="citation add-top-padding">
                    <h1 id="abstract"> Citation </h1>
                    <p> If you use this work or find it helpful, please consider citing: (bibtex) </p>
                    <pre id="codecell0">@inproceedings{fgprompt2023,
&nbsp;author = {Xinyu, Sun and Peihao, Chen and Jugang, Fan and Thomas, H. Li and Jian, Chen and Mingkui, Tan},
&nbsp;title = {FGPrompt: Fine-grained Goal Prompting for Image-goal Navigation},
&nbsp;booktitle = {37th Conference on Neural Information Processing Systems (NeurIPS 2023)},
&nbsp;year = {2023},
} </pre>
                </div> -->

                <b>Acknowledgement</b>: Source code for this page was taken from <a href="https://lerf.io">LERF's website.</a>

                <br>
  
                <a href="https://www.easycounter.com/">
                <img src="https://www.easycounter.com/counter.php?xinyusun"
                border="0" alt="Free Hit Counters"></a>
                <br><a href="https://www.easycounter.com/">Web Site Hit Counters</a>
                    
                
                <br>

            </div>
        </div>
    </div>
    <video id="bouquet" controls="false" loop="" autoplay="" muted="" class="videos" webkit-playsinline="" playsinline="">
        <source src="data/playback_webm/bouquet_concat.webm" type="video/webm">
        <source src="data/playback/bouquet_concat.mp4" type="video/mp4">
    </video>
    <video id="figurines" controls="false" loop="" muted="" class="videos" webkit-playsinline="" playsinline="">
        <source src="data/playback_webm/figurines_concat.webm" type="video/webm">
        <source src="data/playback/figurines_concat.mp4" type="video/mp4">
    </video>
    <video id="kitchen" controls="false" loop="" muted="" class="videos" webkit-playsinline="" playsinline="">
        <source src="data/playback_webm/kitchen_concat.webm" type="video/webm">
        <source src="data/playback/kitchen_concat.mp4" type="video/mp4">
    </video>
    <video id="donuts" controls="false" loop="" muted="" class="videos" webkit-playsinline="" playsinline="">
        <source src="data/playback_webm/donuts_concat.webm" type="video/webm">
        <source src="data/playback/donuts_concat.mp4" type="video/mp4">
    </video>
    <video id="teatime" controls="false" loop="" muted="" class="videos" webkit-playsinline="" playsinline="">
        <source src="data/playback_webm/teatime_concat.webm" type="video/webm">
        <source src="data/playback/teatime_concat.mp4" type="video/mp4">
    </video>
    <video id="bookstore" controls="false" loop="" muted="" class="videos" webkit-playsinline="" playsinline="">
        <source src="data/playback_webm/bookstore_concat.webm" type="video/webm">
        <source src="data/playback/bookstore_concat.mp4" type="video/mp4">
    </video>
    <video id="grocery" class="videos" controls="false" loop="" muted="" webkit-playsinline="" playsinline="">
        <source src="data/playback_webm/veggieaisle_concat.webm" type="video/webm">
        <source src="data/playback/veggieaisle_concat.mp4" type="video/mp4">
    </video>
    <video id="garden" class="videos" controls="false" loop="" muted="" webkit-playsinline="" playsinline="">
        <source src="data/playback_webm/sunnyside_concat.webm" type="video/webm">
        <source src="data/playback/sunnyside_concat.mp4" type="video/mp4">
    </video>
    <video id="shoes" class="videos" controls="false" loop="" muted="" webkit-playsinline="" playsinline="">
        <source src="data/playback_webm/shoerack_concat.webm" type="video/webm">
        <source src="data/playback/shoerack_concat.mp4" type="video/mp4">
    </video>

<deepl-input-controller><template shadowrootmode="open"><link rel="stylesheet" href="chrome-extension://cofdbpoegempjloogbagkncekinflcnj/build/content.css"><div><div class="dl-input-translation-container svelte-ju4595"><div></div></div></div></template></deepl-input-controller></body><grammarly-desktop-integration data-grammarly-shadow-root="true"><template shadowrootmode="open"><style>
      div.grammarly-desktop-integration {
        position: absolute;
        width: 1px;
        height: 1px;
        padding: 0;
        margin: -1px;
        overflow: hidden;
        clip: rect(0, 0, 0, 0);
        white-space: nowrap;
        border: 0;
        -moz-user-select: none;
        -webkit-user-select: none;
        -ms-user-select:none;
        user-select:none;
      }

      div.grammarly-desktop-integration:before {
        content: attr(data-content);
      }
    </style><div aria-label="grammarly-integration" role="group" tabindex="-1" class="grammarly-desktop-integration" data-content="{&quot;mode&quot;:&quot;full&quot;,&quot;isActive&quot;:true,&quot;isUserDisabled&quot;:false}"></div></template></grammarly-desktop-integration></html>